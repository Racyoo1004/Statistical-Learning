---
title: "Stat4620_FinalProject"
author: "Group 8"
date: '2022-12-12'
output: html_document
---

```{r}
library(tidyverse)
library(pls)
library(glmnet)

train <- read.csv("train.ames.csv")
test <- read_csv("test_new.csv")
```

## Part 1: Exploratory Data Analysis
#### Are there any problems with the data?
The dataset contained 81 columns and many missing values. Thus, we wanted to remove the missing values and reduce the dimension to predict models more precisely.

#### What kind of variables do you have?
All of the variables are home features that are used to measure the price. 
In the dataset, there is a mixture of integers, characters, dummy variables, and years. In total, we have 43 character variables and 38 numeric .

#### Is there any missing data?
There is not a ton of missing data, but certain columns have many more missing values than others. Variables such as “Alley” have over one thousand missing values, for example. We should probably filter out these problematic factors.

#### Does there appear to be potentially problematic collinearity amongst the predictor variables?
Variables like RoofMatl, Heating, and LowQualFinSF all have similar responses for all observations, which doesn’t provide too much insight to valuable factors. 

#### Can you detect early signs of what variables are likely to be important in predicting the response?
Naturally, we would assume that common measures would correlate to house prices, like number of bathrooms and bedrooms, lot area, and square footage. Other location variables like neighborhood and distance from railroads and businesses also seem promising in predicting prices. Moreover, these variables don’t have missing values, which helps to predict the response better.

```{r}
data = select_if(train, is.numeric)
data_cor <- cor(data[,colnames(data) != "SalePrice"],train$SalePrice)
data_cor1 <- as.data.frame(data_cor)

ggplot(data=data_cor1, aes(x = V1, y = row.names(data_cor1))) + geom_point(color = "red") + labs(title = "Correlation of Sale Price to Numeric Features", x = "Correlation")
```

We can identify here that the numeric features with highest correlation with sale price are Overall Quality, Living Area, Square Footage,and different Garage and bathroom features. Negative Correlation with features like Year Sold and Overall Condition.

#### What are the key figures or numerical summaries that describe the most important aspects of the data?
```{R}
ggplot(data=train, aes(x=SalePrice)) + geom_histogram(fill="blue", color="black") + labs(x="Sale Price", title="Distribution of Sale Price")
```

Sale price distribution is right skewed: mean= 180,291 median = 163,000.

#### Does your EDA suggest to you what modeling approaches you should aim to try?
Since we've seen that there are many NA values and duplicate responses, we are going to start with dimension reduction.

#### Trimming 
```{r}
# Remove ID column
train <- subset(train, select = -c(Id))

# Remove 6 cols with mostly NA values
train <- subset(train, select = -c(PoolQC,MiscFeature,Alley,Fence,
                                   FireplaceQu,LotFrontage))

# Remove high similar responses (80% of obs have same response) - 20 cols
train <- subset(train, select = -c(Utilities,Street,Condition2,RoofMatl,Heating,
                                   LandSlope,CentralAir,Functional,PavedDrive,
                                   Electrical,GarageCond,LandContour,BsmtCond,
                                   GarageQual,ExterCond,SaleType,Condition1,
                                   BsmtFinType2,BldgType,SaleCondition))
# Remove 22 low-correlated (R<0.4) numeric features
train <- subset(train, select = -c(MSSubClass,LotArea,OverallCond,BsmtFinSF1,
                                   BsmtFinSF2,BsmtUnfSF,X2ndFlrSF,LowQualFinSF,
                                   BsmtFullBath,BsmtHalfBath,HalfBath,
                                   BedroomAbvGr,KitchenAbvGr,WoodDeckSF,
                                   OpenPorchSF,EnclosedPorch,X3SsnPorch,
                                   ScreenPorch,PoolArea,MiscVal,MoSold,YrSold))

# Do the same thing to the test data
test <- subset(test, select = -c(Id,PoolQC,MiscFeature,Alley,Fence,FireplaceQu,
                                 LotFrontage,Utilities,Street,Condition2,
                                 RoofMatl,Heating,LandSlope,CentralAir,
                                 Functional,PavedDrive,Electrical,GarageCond,
                                 LandContour,BsmtCond,GarageQual,ExterCond,
                                 SaleType,Condition1,BsmtFinType2,BldgType,
                                 SaleCondition,MSSubClass,LotArea,OverallCond,
                                 BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,X2ndFlrSF,
                                 LowQualFinSF,BsmtFullBath,BsmtHalfBath,
                                 HalfBath,BedroomAbvGr,KitchenAbvGr,WoodDeckSF,
                                 OpenPorchSF,EnclosedPorch,X3SsnPorch,
                                 ScreenPorch,PoolArea,MiscVal,MoSold,YrSold))

# Remove rows that have NA values
train <- na.omit(train) # Removes 120 rows
test <- na.omit(test) # Removes 127 rows
```

After trimming down the data, both train and test data were reduced from 81 columns to 32. By choosing to omit the NA values, we eliminate rows where an observation has no recorded value. This brings the number of observations in the training data to 1340, and 1320 in the testing set, which is roughly equal still. 

## Part 2: Model Analysis

#### First Models
To examine how the data reacts to simple regressions, we started by using the lm() function in R to run simple liner regression with all variables that were kept after EDA. From this model, the predicted test MSE comes out to 656,529,583 but the model might lack some flexibility. In order to try and further reduce the number of features used, a second linear regression was run with only the significant features from the first model. While this seemed like a good idea, the resulting test MSE went up by about 200 million. These findings pushed our search for a better model down the path of more dimension reduction, to try and find the most important features with more advanced methods. 

```{r}
# PCR with built in cross validation
M3_pcr <- pcr(SalePrice ~ ., data=train, validation="CV")

# Categorical vars get split into dummies for each response type
#   so we can consider up to 123 features

# Plot Cross-Validation error
validationplot(M3_pcr, val.type="MSEP", xlim=c(0,100), ylim=c(0,8*10^9)) + 
  abline(v=11, col="blue")
```

Next, we turned to Principle Component Analysis/Regression to bring the number of features down to a more understandable level. The PCR still has high MSE, but the validation plot shows that we can eliminate quite a few components, and only keep around 11 to achieve the same level of  as with higher numbers.

Additionally, early ridge regressions and LASSOs helped lead to our final model, where we could find an appropriate number of components to consider and reduce dimension to a manageable level. 

For some key categorical variables we are interested in, we need to convert them into numeric variables to train our models. So we assigned these categorical variables numeric labels (1,2,3...)  Then, we do the same for the test set.
```{r,echo=FALSE}
train$KitchenQual <- as.numeric(factor(train$KitchenQual, levels = c('Po','Fa','TA','Gd','Ex'), labels = c(1,2,3,4,5)))
train$GarageType <- as.numeric(factor(train$GarageType, levels = c('Detchd','CarPort','BuiltIn','Basment','Attchd','2Types'), labels = c(1,2,3,4,5,6)))
train$MSZoning<-as.numeric(factor(train$MSZoning, levels=c('A','C','FV','I','RH','RL','RP','RM'), labels=c(1,2,3,4,5,6,7,8)))
train$LotShape<-as.numeric(factor(train$LotShape, levels=c('Reg','IR1'), labels=c(1,2)))
train$LotConfig<-as.numeric(factor(train$LotConfig, levels=c('Inside','Corner','CulDSac','FR2','FR3'), labels=c(1,2,3,4,5)))
train$Neighborhood<-as.numeric(factor(train$Neighborhood, levels=c('Blmngtn','Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr', 'Crawfor', 'Edwards', 'Gilber', 'IDOTRR', 'MeadowV', 'Mitchel', 'Names', 'NoRidge', 'NPkVill', 'NridgHt',
 'NWAmes'	,'OldTown','SWISU','Sawyer','SawyerW','Somerst','StoneBr','Timber','Veenker'),
labels=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)))
train$HouseStyle<-as.numeric(factor(train$HouseStyle, levels=c('1Story','1.5Fin',	'1.5Unf','2Story',	'2.5Fin',	'2.5Unf',	'SFoyer',	'SLvl'	), labels=c(1,2,3,4,5,6,7,8)))
train$RoofStyle<-as.numeric(factor(train$RoofStyle, levels=c('Flat','Gable','Gambrel','Hip','Mansard','Shed'), labels=c(1,2,3,4,5,6)))
train$MasVnrType<-as.numeric(factor(train$MasVnrType,levels=c('BrkCmn','BrkFace','CBlock','None','Stone'),label=c(1,2,3,4,5)))
train$ExterQual<-as.numeric(factor(train$ExterQual,levels=c('Ex','Gd','TA','Fa','Po'),label=c(1,2,3,4,5)))
train$Foundation<-as.numeric(factor(train$Foundation,levels=c('BrkTil','CBlokc','Pconc','Slab','Stone','Wood'),label=c(1,2,3,4,5,6)))
train$BsmtQual<-as.numeric(factor(train$BsmtQual,levels=c('Ex','Gd','TA','Fa','Po'),label=c(1,2,3,4,5)))
train$BsmtExposure<-as.numeric(factor(train$BsmtExposure,levels=c('Gd','Av','Mn','No'),label=c(1,2,3,4)))
train$BsmtFinType1<-as.numeric(factor(train$BsmtFinType1,levels=c('GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ','Unf'),label=c(1,2,3,4,5,6)))
train$HeatingQC<-as.numeric(factor(train$HeatingQC,levels=c('Ex','Gd','TA','Fa','Po'),label=c(1,2,3,4,5)))
```

```{r,echo=FALSE}
test$KitchenQual <- as.numeric(factor(test$KitchenQual, levels = c('Po','Fa','TA','Gd','Ex'), labels = c(1,2,3,4,5)))
test$GarageType <- as.numeric(factor(test$GarageType, levels = c('Detchd','CarPort','BuiltIn','Basment','Attchd','2Types'), labels = c(1,2,3,4,5,6)))
test$MSZoning<-as.numeric(factor(test$MSZoning, levels=c('A','C','FV','I','RH','RL','RP','RM'), labels=c(1,2,3,4,5,6,7,8)))
test$LotShape<-as.numeric(factor(test$LotShape, levels=c('Reg','IR1'), labels=c(1,2)))
test$LotConfig<-as.numeric(factor(test$LotConfig, levels=c('Inside','Corner','CulDSac','FR2','FR3'), labels=c(1,2,3,4,5)))
test$Neighborhood<-as.numeric(factor(test$Neighborhood, levels=c('Blmngtn','Blueste','BrDale','BrkSide', 'ClearCr', 'CollgCr','Crawfor',  'Edwards', 'Gilber','IDOTRR',   'MeadowV', 'Mitchel', 'Names', 'NoRidge', 'NPkVill', 'NridgHt', 'NWAmes',  'OldTown','SWISU', 'Sawyer', 'SawyerW','Somerst', 'StoneBr','Timber',  'Veenker'),
labels=c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25)))

test$HouseStyle<-as.numeric(factor(test$HouseStyle, levels=c('1Story', '1.5Fin',	 '1.5Unf', '2Story',	 '2.5Fin', '2.5Unf',	 'SFoyer',	'SLvl'	), labels=c(1,2,3,4,5,6,7,8)))
test$RoofStyle<-as.numeric(factor(test$RoofStyle, levels=c('Flat','Gable','Gambrel','Hip','Mansard','Shed'), labels=c(1,2,3,4,5,6)))
test$ExterQual<-as.numeric(factor(test$ExterQual,levels=c('Ex','Gd','TA','Fa','Po'),label=c(1,2,3,4,5)))

test$BsmtQual<-as.numeric(factor(test$BsmtQual,levels=c('Ex','Gd','TA','Fa','Po'),label=c(1,2,3,4,5)))



```



Converting some of the key categorical variables too numeric for use in out final
models

```{r}
train <- subset(train, select = -c(Foundation,LotConfig,Exterior1st,Exterior2nd,MasVnrType,MasVnrArea,BsmtExposure,BsmtFinType1,HeatingQC,X1stFlrSF,GarageFinish,GarageCars))
```

```{r}
test <- subset(test, select = -c(Foundation,LotConfig,Exterior1st,Exterior2nd,MasVnrType,MasVnrArea,BsmtExposure,BsmtFinType1,HeatingQC,X1stFlrSF,GarageFinish,GarageCars))
```

Further trimming of test and training sets, getting rid of repetitive (multicollinearilty) or useless categorical variables for optimal final model performance

```{r}
train<- na.omit(train)
test<- na.omit(test)
xtrain = model.matrix(SalePrice~., data=train)[,-1]
y = train$SalePrice
xtest = model.matrix(SalePrice~.,data=test)[,-1]
ytest = test$SalePrice
```

```{r}
ridge.cv = cv.glmnet(xtrain,y,alpha=0)    #Finding optimal lambda value for ridge 
plot(ridge.cv)
```

```{r}
minlam = ridge.cv$lambda.min
Ridge <- glmnet(xtrain,y,alpha=0,lambda=minlam)
Ridge.pred <- predict(Ridge, xtest)
Ridge.error <- mean((Ridge.pred-ytest)^2)
Ridge.error
```

Ridge Regression with our final 19 predictors gives MSE of **1,189,653,580**.

```{r}
lasso.cv = cv.glmnet(xtrain,y, alpha = 1)   #Finding optimal lambda value for lasso
plot(lasso.cv)
```

```{r}
minlam2 = lasso.cv$lambda.min
Lasso <- glmnet(xtrain, y, alpha=1, lambda = minlam2)
Lasso.pred <- predict(Lasso,xtest)
Lasso.error <- mean((Lasso.pred-ytest)^2)
Lasso.error

```

Lasso Regression with our final 19 predictors gives MSE of **1,184,991,903**.

```{r}
actuals_predsLASSO <- data.frame(cbind(actual=ytest, predicted=Lasso.pred))
ggplot(actuals_predsLASSO, aes(x=actual, y=s0)) + geom_point(shape=1) + geom_smooth(method = "lm",col="red") + labs(x="Actual Saleprice",y="Predicted Saleprice",title="Actual vs. Predicted Saleprices: LASSO model") + annotate("text",x=200000,y=400000,label="Overestimate",size=4,col="red") +
  annotate("text",x=400000,y=200000,label="Underestimate",size=4,col="red")
```

We can see here that our predictions are strongly correlated with the actual values in our testing set.

```{r}
cor(actuals_predsLASSO)
```

Mathematically speaking, there is a near 92% correlation between the two, and thus giving us more confidence in the accuracy of our model. 


#### Mathematical Description

Our LASSO model minimizes the following quantity: $$\sum_{i=1}^{n} (SalePrice_i - \beta_0 - \sum_{j=1}^p \beta_i x_{ij})^2 + \lambda \sum_{j=1}^p |\beta_j|,$$ 
<center>
or quite simply
</center>
$$RSS + \lambda \sum_{j=1}^p |\beta_j|,$$
Where $SalePrice_i$ is the the sale price of the $i^{th}$ house, 

$\beta_0$ is the intercept coefficient,

$\beta_j$'s represent each predictor's coefficient,

$x_{ij}$'s are the values of each predictor (0 or 1 for categorical variables),

and $\lambda$ is a tuning parameter that will shrink all coefficients toward zero as it increases.



---
## Conclusion

Starting with exploratory data analysis, we could see how data is stored in the dataset and what each feature means in this dataset. After EDA, we did some data cleaning such as removing NA values, duplicate responses, and unnecessary features to come up with better predictions with various models.

First, linear models will be rigid and listen to too much noise and ultimately overfit. This leads to high variance and low bias. As we moved through the model fitting process, we wanted a more flexible model that didn't make the variance skyrocket. After using Ridge Regression and LASSO techniques, we've conclude that the LASSO model is best due to its lesser MSE. Thus, we can better control the flexibility with the penalty parameter $\lambda$ to control bias and variance and understand the tradeoff between the two. Additionally, with such a large dataset, the automatic variable selection and dimension reduction are useful in determining important features that help predict a home sale price.

